Exercise 1: Assume we have explained steps 1 and 2 as they are explained in the project spec. We continue the explanation from step 3. 

Step 3: simpledb.Parser.handleQueryStatement()

This method creates a new query object which will eventually be returned to processNextStatement() which will execute it. This method takes in a ZQuery which it passes  to parseQueryLogicalPlan() in order to get a logical plan. Once, a logical plan is obtained, a physical plan is created by calling LogicalPlan.physicalPlan(). The query’s logical and physical plans are then set accordingly and the query is returned so that it could be executed. 

Step 4: simpledb.Parser.parseQueryLogicalPlan()  

This method instantiates a new LogicalPlan object and then iterates through each part of the query and sets the appropriate fields in the logical plan. For example, as part of parsing the query, this method looks at the FROM clause of the query and for each table adds a scan to the logical plan by calling addScan(). Similarly, for the WHERE clause the function calls simpledb.Parser.processExpression() which examines the where clause and adds any appropriate filter and join operations to the logical plan via addFilter() and addJoin() methods. Next, the GROUP BY, AGGREGATE, and ORDER BY clauses of the query are parsed in a similar way (i.e calling LogicalPlan.addAggregate(), addProjectField(), and addOrderBy(), which in turn set the corresponding fields in the logical plan instance). Once each part of the query has been parsed, the resulting logical plan is returned.

Step 5: simpledb.LogicalPlan.physicalPlan()

Using the logical plan created in the previous plan, the physicalPlan() method is called to retrieve a DbIterator representing the optimal ordering of execution for the plan. To do this, the code iterates through tables in the FROM clause and obtains a new SeqScan for each table. This allows statistics to be calculated and results to be returned. The physical plan then applies a predicate to each node in the WHERE clause via a new Filter object. Similarly, subPlanMap helps determine the best outcome by computing statistics on the tables. PhysicalPlan also instantiates a JoinOptimizer to compute an optimal join ordering for the query. Finally the output is a relation, created by iterating through fields in the SELECT clause of the query. Aggregates and ordering operations are then calculated after this step if the current column is used in the AGGREGATE or ORDER BY calls. The result is returned as a Project object having the corresponding fields, fieldTypes, and iterators to the columns in the SELECT clause of the query. 

Step 6: simpledb.Parser.processNextStatement()
The previous step concludes the initial call to handleQueryStatement() from Step 3, and hence the query in Parser.handleQueryStatement has a physical plan it can now execute. Query.execute() is called on 538 of simpledb.Parser. Which concludes the life of a query in SimpleDB. Please note there is special error handling involved after executing the query to either commit or abort the particular transaction associated with the query.  

Exercise 6:
We used sample 0.01

6.1:
The query plan that our optimizer selected was :
                            π(d.fname,d.lname),card:1
                            |
                            ⨝(a.id=c.pid),card:1
  __________________________|___________________________
  |                                                    |
  σ(a.lname=Spicer),card:1                          ⨝(m.mid=c.mid),card:29729
  |                                    ________________|_________________
  σ(a.fname=John),card:1               |                                |
  |                                    ⨝(d.id=m.did),card:2791          |
  |                           _________|_________                       |
  |                           |                 |                scan(Casts c)
scan(Actor a)               scan(Director d)  scan(Movie_Director m)

The cost of joins were:
Join m:d (Cost =2605597.0, card = 1)
Join c:m (Cost =2632729.0, card = 29729)
Join a:c (Cost =2632729.0, card = 29729)
a (Cost = 2603000.0, card = 0)
c (Cost = 1026000.0, card = 29729)
m (Cost = 6000.0, card = 2791)
d (Cost = 174000.0, card = 2597)

Our optimizer selected this plan because the cardinality of join m:d (before any selections) was the lowest, so it would be best to join m:d first. The cardinalities of join c:m and join a:c were the same, so it didn’t matter what order those were joined in. It is also better to do selections before joins, and to make the smaller relation the outer relation in the join.

6.2:
Our query: select m.name, m.year, g.genre from Movie m, Genre g where m.year='2011' and g.mid = m.id;

The query plan is:
             π(m.name,m.year,g.genre),card:4499
              |
             ⨝(m.id=g.mid),card:4499
  ____________|_____________
  |                        |
 σ(m.year=2011),card:6    |
  |                        |
scan(Movie m)            scan(Genre g)

Our optimizer generated this plan because it is more optimal to do a selection before a join, and it is more optimal for the relation with the smaller cardinality to be the outer relation in the join.

API Changes: We did not make any significant changes to the existing API. We found the API relatively robust and were able to complete the project with the existing API setup. We did add a helper method in IntHistogram that maps values to a specific bucket. 

Missing or incomplete elements: We implemented all parts that were required in the project specification.

Time Spent: As a group, we spent about a combined 20 man hours on this project. One part that we were unclear about but was cleared up via Piazza was how to handle the case where the bucket size of less than 1. This was contributing to a weird error where the computed selectivity was not passing the unit test and was different each time due to the random distribution of values. We solved this problem by rounding up any bucket widths less than one. An interesting design decision we had to make was in how to compute the TableStats for a table. We came up with two different solutions. One that would scan each tuple one by one and simultaneously keep track of histograms for each column. Or the second option, to iterate through each column and do a sequential scan of the tuples. We decided to go with the first method since we require two sequential scans of the tuples (O(n)) whereas the second option would require a number of sequential scans proportional to the number of columns (O(n^2)). However, with the first option we must allocate enough memory to keep track of each column’s histogram (O(n^2) space). This was a decision to sacrifice space complexity in favor of time complexity.

